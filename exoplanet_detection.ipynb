{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL IMPORTS AND IMPORTANT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%pylab inline\n",
    "\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
    "\n",
    "raw_data = np.loadtxt('exoTrain.csv', skiprows = 1, delimiter = ',')\n",
    "x_train = raw_data[:, 1:]\n",
    "y_train = raw_data[:, 0, np.newaxis] - 1.\n",
    "raw_data = np.loadtxt('exoTest.csv', skiprows = 1, delimiter = ',')\n",
    "x_test = raw_data[:, 1:]\n",
    "y_test = raw_data[:, 0, np.newaxis] - 1.\n",
    "del raw_data\n",
    "\n",
    "flux_data = pd.read_csv('exoTrain.csv', index_col = 0)\n",
    "\n",
    "def reduce_upper_outliers(df, reduce = 0.01, half_width = 4):\n",
    "    length = len(df.iloc[0,:])\n",
    "    remove = int(length*reduce)\n",
    "    for i in df.index.values:\n",
    "        values = df.loc[i,:]\n",
    "        sorted_values = values.sort_values(ascending = False)\n",
    "        for j in range(remove):\n",
    "            idx = sorted_values.index[j]\n",
    "            new_val = 0 \n",
    "            count = 0\n",
    "            idx_num = int(idx[5:])\n",
    "            for k in range(2*half_width+1):\n",
    "                idx2 = idx_num + k - half_width\n",
    "                if idx2 <1 or idx2 >= length or idx_num == idx2:\n",
    "                    continue\n",
    "                new_val += values['FLUX-'+str(idx2)]\n",
    "\n",
    "                count += 1\n",
    "            new_val /= count # count will always be positive here\n",
    "            if new_val < values[idx]: # just in case there's a few persistently high adjacent values\n",
    "                df.set_value(i,idx,new_val)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_light_curve(dataset, index):\n",
    "  plt.figure()\n",
    "  plt.plot(np.array(dataset[index:index+1])[0])\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNDERSTANDING THE DATA WITH PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flux_data = pd.read_csv('exoTrain.csv', index_col = False)\n",
    "flux_data.LABEL=flux_data.LABEL-1 \n",
    "flux_data.head()\n",
    "print(flux_data.info)\n",
    "print(flux_data.describe)\n",
    "labels = flux_data.LABEL\n",
    "flux_data = flux_data.drop('LABEL',axis=1) #drop the labels from the data frame, leaving only the flux data\n",
    "non_exo_data=flux_data.loc[labels==0] #select only rows with label 0\n",
    "exo_data = flux_data.loc[labels==1] \n",
    "print(len(exo_data))\n",
    "print(len(non_exo_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising Light Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_light_curve(exo_data, 18)\n",
    "plot_light_curve(non_exo_data, 500)\n",
    "plot_light_curve(non_exo_data, 500)\n",
    "#Run this to print a random light curve\n",
    "import random\n",
    "val=random.randint(0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting one period of the exoplanet orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12 #change these values\n",
    "t_0 = 430 #change these values\n",
    "period = 1193 #change these values\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "light_curve=np.array(exo_data.loc[index])\n",
    "plt.plot(light_curve)\n",
    "plt.title('Box Covering One Period of Exoplanet Transit')\n",
    "plt.gca().add_patch(Rectangle((t_0, -510), period, 700, linewidth=1,edgecolor='r',facecolor='none'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(light_curve[t_0: t_0+period])\n",
    "plt.title('Plot of Just One Period')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folding Light Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the curve starting from Period 1\n",
    "start_period_1 = t_0 #FILL ME IN - time of first transit\n",
    "plt.plot(light_curve[start_period_1:]) #plots the first curve\n",
    "\n",
    "#Plot the curve starting from Period 2\n",
    "start_period_2 = t_0 + period #initial time + the period of orbit\n",
    "plt.plot(light_curve[start_period_2:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXOPLANET CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
    "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import  metrics\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_train = pd.read_csv('exoTrain.csv')\n",
    "df_train.LABEL = df_train.LABEL -1\n",
    "df_test = pd.read_csv('exoTest.csv')\n",
    "df_test.LABEL = df_test.LABEL - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "df_train.head()\n",
    "#for exoplanets\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    ax.scatter(np.arange(3197),df_train[df_train['LABEL'] == 1].iloc[i,1:],s=1)\n",
    "#for non exoplanets\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    ax.scatter(np.arange(3197),df_train[df_train['LABEL']==0].iloc[i,1:],s=1)\n",
    "#histograms of exoplanet stars\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    plt.xlabel(\"Flux\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    df_train[df_train['LABEL']==1].iloc[i,1:].hist(bins=40)\n",
    "#histograms of non-exoplanet stars\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    plt.xlabel(\"Flux\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    df_train[df_train['LABEL']==0].iloc[i,1:].hist(bins=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Exoplanets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exoplanets = df_train.loc[df_train['LABEL'] == 1]\n",
    "df_train.loc[df_train['LABEL'] == 0]\n",
    "df_test.loc[df_test['LABEL'] == 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(model, train_X, train_y, test_X, test_y):\n",
    "    \"\"\"\n",
    "    Helper function to help interpret and model performance.\n",
    "\n",
    "    Args:\n",
    "    model: estimator instance\n",
    "    train_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for model training.\n",
    "    train_y : array-like of shape (n_samples,)\n",
    "    Target values for model training.\n",
    "    test_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for model testing.\n",
    "    test_y : array-like of shape (n_samples,)\n",
    "    Target values for model testing.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Model Results\")\n",
    "    print(\"\")\n",
    "    print(\"Training:\")\n",
    "    ConfusionMatrixDisplay.from_estimator(model,train_X,train_y)\n",
    "    plt.show()\n",
    "    print(\"Testing:\")\n",
    "    ConfusionMatrixDisplay.from_estimator(model,test_X,test_y)\n",
    "\n",
    "def reset(train,test):\n",
    "    train_X = train.drop('LABEL', axis=1)\n",
    "    train_y = train['LABEL'].values\n",
    "    test_X = test.drop('LABEL', axis=1)\n",
    "    test_y = test['LABEL'].values\n",
    "    return train_X,train_y,test_X,test_y\n",
    "\n",
    "train_X,train_y,test_X,test_y = reset(df_train, df_test)\n",
    "n_neighbors = 3\n",
    "model = KNeighborsClassifier(n_neighbors)\n",
    "model.fit(train_X, train_y)\n",
    "train_predictions = model.predict(train_X)\n",
    "test_predictions = model.predict(test_X)\n",
    "print(accuracy_score(train_y, train_predictions))\n",
    "print(accuracy_score(test_y, test_predictions))\n",
    "analyze_results(model, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "model = LogisticRegression(max_iter = max_iter)\n",
    "model.fit(train_X, train_y)\n",
    "train_predictions = model.predict(train_X)\n",
    "test_predictions = model.predict(test_X)\n",
    "print(accuracy_score(train_y, train_predictions))\n",
    "print(accuracy_score(test_y, test_predictions))\n",
    "analyze_results(model, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to define helper functions\n",
    "# Helper functions that we can run for the three augmentation functions that will be used, but not explored in depth\n",
    "\n",
    "def smote(a,b):\n",
    "    model = SMOTE()\n",
    "    X,y = model.fit_resample(a, b)\n",
    "    return X,y\n",
    "\n",
    "def savgol(df1,df2):\n",
    "    x = savgol_filter(df1,21,4,deriv=0)\n",
    "    y = savgol_filter(df2,21,4,deriv=0)\n",
    "    return x,y\n",
    "\n",
    "def fourier(df1,df2):\n",
    "    train_X = np.abs(np.fft.fft(df1, axis=1))\n",
    "    test_X = np.abs(np.fft.fft(df2, axis=1))\n",
    "    return train_X,test_X\n",
    "\n",
    "def norm(df1,df2):\n",
    "    train_X = normalize(df1)\n",
    "    test_X = normalize(df2)\n",
    "    return train_X,test_X\n",
    "\n",
    "def robust(df1,df2):\n",
    "    scaler = RobustScaler()\n",
    "    train_X = scaler.fit_transform(df1)\n",
    "    test_X = scaler.transform(df2)\n",
    "    return train_X,test_X\n",
    "\n",
    "def norm(train_X, test_X):\n",
    "    \"\"\"\n",
    "    Helper function to normalize inputs train_X and test_X\n",
    "    \"\"\"\n",
    "    return norm_train_X, norm_test_X\n",
    "\n",
    "fourier_train_X, fourier_test_X = fourier(train_X, test_X)\n",
    "savgol_train_X, savgol_test_X = savgol(fourier_train_X, fourier_test_X)\n",
    "\n",
    "#####TODO#########\n",
    "norm_train_X, norm_test_X = norm(savgol_train_X, savgol_test_X)\n",
    "##################\n",
    "\n",
    "robust_train_X, robust_test_X = robust(norm_train_X, norm_test_X)\n",
    "smote_train_X,smote_train_y = smote(robust_train_X, train_y)\n",
    "\n",
    "#combining real and augmented data\n",
    "aug_train_X, new_X_test_data, aug_train_y, new_y_test_data = train_test_split(smote_train_X, smote_train_y, test_size=0.3)\n",
    "aug_test_X = np.concatenate((robust_test_X, new_X_test_data), axis=0)\n",
    "aug_test_y = np.concatenate((test_y, new_y_test_data), axis=0)\n",
    "\n",
    "#load helper functions and variables\n",
    "def visualize_data(X, y, indicies):\n",
    "  stars = indicies\n",
    "\n",
    "  fig = plt.figure(figsize=(24,500))\n",
    "  x = np.array(range(3197))\n",
    "  for i in range(0,len(stars)):\n",
    "      ax = fig.add_subplot(100,5,1+i)\n",
    "      ax.set_title('Data '+\"I:\"+str(stars[i])+\" Label:\"+str(y[stars[i]]))\n",
    "      ax.scatter(x, X[stars[i],:],s=1)\n",
    "\n",
    "robust_train_y_with_exoplanet_indicies = train_y.nonzero()[0]\n",
    "robust_train_y_with_no_exoplanet_indicies = (train_y-1).nonzero()[0]\n",
    "\n",
    "aug_train_y_with_exoplanet_indicies = aug_train_y.nonzero()[0]\n",
    "aug_train_y_with_no_exoplanet_indicies = (aug_train_y-1).nonzero()[0]\n",
    "\n",
    "#visualize original, preprocessed data with exoplanets\n",
    "print(\"Orginal, newly modified data with exoplanets:\")\n",
    "visualize_data(robust_train_X, train_y, robust_train_y_with_exoplanet_indicies[:5])\n",
    "\n",
    "# visualize newly generated data containing exoplanets\n",
    "print(\"Augmented data with Exoplanets:\")\n",
    "visualize_data(aug_train_X, aug_train_y, aug_train_y_with_exoplanet_indicies[:5])\n",
    "\n",
    "#visualize original, modified data with no Exoplanets\n",
    "print(\"Orginal, newly modified data with no exoplanets:\")\n",
    "visualize_data(robust_train_X, train_y, robust_train_y_with_no_exoplanet_indicies[:5])\n",
    "\n",
    "#visualize newly generated data containing no exoplanets\n",
    "print(\"Augmented data with no Exoplanets:\")\n",
    "visualize_data(aug_train_X, aug_train_y, aug_train_y_with_no_exoplanet_indicies[:5])\n",
    "\n",
    "print(\"train_y len:\", len(aug_train_y))\n",
    "\n",
    "number_of_exoplanet_samples = np.count_nonzero(aug_train_y == 1)\n",
    "number_of_non_exoplanet_samples = np.count_nonzero(aug_train_y == 0)\n",
    "\n",
    "print(\"exoplanet samples in train_y:\", number_of_exoplanet_samples)\n",
    "print(\"non-exoplanet samples in train_y:\", number_of_non_exoplanet_samples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(aug_train_X, aug_train_y)\n",
    "analyze_results(model=model, train_X=aug_train_X, train_y=aug_train_y, test_X=aug_test_X, test_y=aug_test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVING MODEL FOR CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import  metrics\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "df_train = pd.read_csv('exoTrain.csv')\n",
    "df_train.LABEL = df_train.LABEL -1\n",
    "df_test = pd.read_csv('exoTest.csv')\n",
    "df_test.LABEL = df_test.LABEL - 1\n",
    "\n",
    "def plot_graphs(history, best):\n",
    "\n",
    "  plt.figure(figsize=[10,4])\n",
    "  # summarize history for accuracy\n",
    "  plt.subplot(121)\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "  # summarize history for loss\n",
    "  plt.subplot(122)\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "def analyze_results(model, train_X, train_y, test_X, test_y):\n",
    "    \"\"\"\n",
    "    Helper function to help interpret and model performance.\n",
    "\n",
    "    Args:\n",
    "    model: estimator instance\n",
    "    train_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for model training.\n",
    "    train_y : array-like of shape (n_samples,)\n",
    "    Target values for model training.\n",
    "    test_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for model testing.\n",
    "    test_y : array-like of shape (n_samples,)\n",
    "    Target values for model testing.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Model Results\")\n",
    "    print(\"\")\n",
    "    print(\"Training:\")\n",
    "    if type(model) == keras.engine.sequential.Sequential:\n",
    "      train_predictions = model.predict(train_X)\n",
    "      train_predictions = (train_predictions > 0.5)\n",
    "      cm = confusion_matrix(train_y, train_predictions)\n",
    "      labels = [0, 1]\n",
    "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
    "      fig = plt.figure()\n",
    "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
    "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
    "      plt.title('Confusion Matrix - TestData')\n",
    "      plt.ylabel('True label')\n",
    "      plt.xlabel('Predicted label')\n",
    "      plt.show()\n",
    "    else:\n",
    "      plt.close()\n",
    "      ConfusionMatrixDisplay.from_estimator(model,train_X,train_y)\n",
    "      plt.show()\n",
    "\n",
    "    print(\"Testing:\")\n",
    "    if type(model) == keras.engine.sequential.Sequential:\n",
    "      test_predictions = model.predict(test_X)\n",
    "      test_predictions = (test_predictions > 0.5)\n",
    "      cm = confusion_matrix(test_y, test_predictions)\n",
    "      labels = [0, 1]\n",
    "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
    "      fig = plt.figure()\n",
    "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
    "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
    "      plt.title('Confusion Matrix - TestData')\n",
    "      plt.ylabel('True label')\n",
    "      plt.xlabel('Predicted label')\n",
    "      plt.show()\n",
    "    else:\n",
    "      ConfusionMatrixDisplay.from_estimator(model,test_X,test_y)\n",
    "\n",
    "def reset(train,test):\n",
    "    train_X = train.drop('LABEL', axis=1)\n",
    "    train_y = train['LABEL'].values\n",
    "    test_X = test.drop('LABEL', axis=1)\n",
    "    test_y = test['LABEL'].values\n",
    "    return train_X,train_y,test_X,test_y\n",
    "\n",
    "train_X,train_y,test_X,test_y = reset(df_train, df_test)\n",
    "\n",
    "#PREPROCESSING DATA\n",
    "def smote(a,b):\n",
    "    model = SMOTE()\n",
    "    X,y = model.fit_resample(a, b)\n",
    "    return X,y\n",
    "\n",
    "def savgol(df1,df2):\n",
    "    x = savgol_filter(df1,21,4,deriv=0)\n",
    "    y = savgol_filter(df2,21,4,deriv=0)\n",
    "    return x,y\n",
    "\n",
    "def fourier(df1,df2):\n",
    "    train_X = np.abs(np.fft.fft(df1, axis=1))\n",
    "    test_X = np.abs(np.fft.fft(df2, axis=1))\n",
    "    return train_X,test_X\n",
    "\n",
    "def norm(df1,df2):\n",
    "    train_X = normalize(df1)\n",
    "    test_X = normalize(df2)\n",
    "    return train_X,test_X\n",
    "\n",
    "def robust(df1,df2):\n",
    "    scaler = RobustScaler()\n",
    "    train_X = scaler.fit_transform(df1)\n",
    "    test_X = scaler.transform(df2)\n",
    "    return train_X,test_X\n",
    "\n",
    "fourier_train_X, fourier_test_X = fourier(train_X, test_X)\n",
    "savgol_train_X, savgol_test_X = savgol(fourier_train_X, fourier_test_X)\n",
    "norm_train_X, norm_test_X = norm(savgol_train_X,savgol_test_X)\n",
    "robust_train_X, robust_test_X = robust(norm_train_X, norm_test_X)\n",
    "smote_train_X,smote_train_y = smote(robust_train_X, train_y)\n",
    "\n",
    "# Here we're adding the generated, augmented data onto the testing data\n",
    "aug_train_X, new_X_test_data, aug_train_y, new_y_test_data = train_test_split(smote_train_X, smote_train_y, test_size=0.3)\n",
    "aug_test_X = np.concatenate((robust_test_X, new_X_test_data), axis=0)\n",
    "aug_test_y = np.concatenate((test_y, new_y_test_data), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(10,), random_state=1, max_iter=300)\n",
    "model.fit(aug_train_X, aug_train_y)\n",
    "train_predictions = model.predict(aug_train_X)\n",
    "test_predictions = model.predict(aug_test_X)\n",
    "print(accuracy_score(aug_train_y, train_predictions))\n",
    "print(accuracy_score(aug_test_y, test_predictions))\n",
    "analyze_results(model, aug_train_X, aug_train_y, aug_test_X, aug_test_y)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation = 'relu', input_shape = (3197,)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(aug_train_X, aug_train_y, batch_size=64, epochs=20, verbose=1, validation_data=(aug_test_X, aug_test_y), shuffle=True)\n",
    "performance = model.evaluate(aug_test_X, aug_test_y, batch_size=64)\n",
    "plot_graphs(history, performance)\n",
    "analyze_results(model, aug_train_X, aug_train_y, aug_test_X, aug_test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_aug_train_X = np.expand_dims(aug_train_X, axis=2)\n",
    "cnn_aug_test_X = np.expand_dims(aug_test_X, axis=2)\n",
    "cnn_aug_train_y = aug_train_y\n",
    "cnn_aug_test_y = aug_test_y\n",
    "\n",
    "cnn_train_X = np.expand_dims(train_X, axis=2)\n",
    "cnn_test_X = np.expand_dims(test_X, axis=2)\n",
    "cnn_train_y = train_y\n",
    "cnn_test_y = test_y\n",
    "\n",
    "print(cnn_aug_train_X.shape)\n",
    "print(cnn_aug_test_X.shape)\n",
    "print(cnn_aug_train_y.shape)\n",
    "print(cnn_aug_test_y.shape)\n",
    "\n",
    "model = Sequential()\n",
    "input_shape = [3197, 1]\n",
    "\n",
    "model.add(Conv1D(8, 5, activation = \"relu\", padding = \"same\", input_shape = input_shape))\n",
    "model.add(MaxPooling1D(pool_size = 4, strides = 4, padding = \"same\"))\n",
    "model.add(Conv1D(16, 3, activation = \"relu\", padding = \"same\"))\n",
    "model.add(MaxPooling1D(pool_size = 4, strides = 4, padding = \"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "validation_data = (cnn_aug_test_X, cnn_aug_test_y)\n",
    "verbose = 1\n",
    "shuffle = True\n",
    "\n",
    "history = model.fit(cnn_aug_train_X, cnn_aug_train_y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                            validation_data=validation_data, shuffle=shuffle)\n",
    "performance = model.evaluate(cnn_aug_test_X, cnn_aug_test_y, batch_size=batch_size)\n",
    "plot_graphs(history, performance)\n",
    "\n",
    "analyze_results(model=model, train_X=cnn_aug_train_X, train_y=cnn_aug_train_y, test_X=cnn_aug_test_X, test_y=cnn_aug_test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
